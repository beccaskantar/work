{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tutorial courtesy of https://stackabuse.com/image-recognition-in-python-with-tensorflow-and-keras/\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
    "from tensorflow.python.keras.layers.convolutional import Conv2D, MaxPooling2D \n",
    "from tensorflow.python.keras.constraints import maxnorm\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "(xtrain, ytrain), (xtest, ytest) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the inputs from 0-255 to between 0 and 1 by dividing by 255\n",
    "xtrain = xtrain.astype('float32')\n",
    "xtest = xtest.astype('float32')\n",
    "xtrain = xtrain / 255.0\n",
    "xtest = xtest / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "ytrain = np_utils.to_categorical(ytrain)\n",
    "ytest = np_utils.to_categorical(ytest)\n",
    "classNum = ytest.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#Convolutional layer runs convolutional filters on inputs\n",
    "#32 filters of size 3x3, specifies shape for first layer\n",
    "model.add(Conv2D(32, (3, 3), input_shape=xtrain.shape[1:], padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#Dropout layer prevents overfitting\n",
    "#Drops 20% of existing connections\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Batch normalization normalizes input\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#Another convolutional layer with larger filter size\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#Pooling layer compresses image\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#Repeat layers again\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten data into one vector\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Dense layers form collections of neurons that form objects\n",
    "model.add(Dense(256, kernel_constraint=maxnorm(3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "    \n",
    "model.add(Dense(128, kernel_constraint=maxnorm(3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#Final layer outputs a 10 neuron vector storing probability of each class\n",
    "#Softmax activation selects neuron with highest probability as output\n",
    "model.add(Dense(classNum))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics= ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 137s 3ms/sample - loss: 1.4232 - acc: 0.4966 - val_loss: 1.1434 - val_acc: 0.5800\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 137s 3ms/sample - loss: 1.0237 - acc: 0.6378 - val_loss: 0.9456 - val_acc: 0.6687\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 138s 3ms/sample - loss: 0.9346 - acc: 0.6730 - val_loss: 0.9203 - val_acc: 0.6686\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 138s 3ms/sample - loss: 0.8942 - acc: 0.6877 - val_loss: 0.8001 - val_acc: 0.7220\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 138s 3ms/sample - loss: 0.8690 - acc: 0.6991 - val_loss: 0.7540 - val_acc: 0.7324\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 138s 3ms/sample - loss: 0.8395 - acc: 0.7088 - val_loss: 0.8055 - val_acc: 0.7217\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 139s 3ms/sample - loss: 0.8220 - acc: 0.7140 - val_loss: 0.7555 - val_acc: 0.7331\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 139s 3ms/sample - loss: 0.8064 - acc: 0.7181 - val_loss: 0.7903 - val_acc: 0.7254\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 139s 3ms/sample - loss: 0.7881 - acc: 0.7266 - val_loss: 0.8221 - val_acc: 0.7153\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 141s 3ms/sample - loss: 0.7800 - acc: 0.7290 - val_loss: 0.7565 - val_acc: 0.7372\n"
     ]
    }
   ],
   "source": [
    "seed = 21\n",
    "np.random.seed(seed)\n",
    "with tf.device('/device:GPU:0'):\n",
    "    model.fit(xtrain, ytrain, validation_data=(xtest,ytest), epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.72%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(xtest, ytest, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
